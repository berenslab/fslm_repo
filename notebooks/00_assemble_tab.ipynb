{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sbi_feature_importance.experiment_helper import SimpleDB\n",
    "from sbi_feature_importance.utils import extract_tags, skip_dims\n",
    "from sbi_feature_importance.analysis import compare_kls\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_db = SimpleDB(\"../results/fig2\", \"r\")\n",
    "HH_db = SimpleDB(\"../results/fig3\", \"r\")\n",
    "HHdata_db = SimpleDB(\"../results/HH1M\", \"r\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model & Method & Training time & Sampling time & Total time & KL \\\\\n",
      "\\hline\n",
      "\\hline\n",
      "  & NLE & 3.57 $\\pm$ 0.79 & 0.33 $\\pm$ 0.04 & 3.89 $\\pm$ 0.80 & 0.06 $\\pm$ 0.10 \\\\\n",
      "  & FSLM & 0.64 $\\pm$ 0.21 & 0.66 $\\pm$ 0.10 & 1.30 $\\pm$ 0.26 & 0.04 $\\pm$ 0.09 \\\\\n",
      "\\hline\n",
      "\\hline\n",
      "  & NLE & 84.20 $\\pm$ 9.28 & 10.34 $\\pm$ 0.54 & 94.53 $\\pm$ 9.24 & 1.90 $\\pm$ 3.99 \\\\\n",
      "  & FSLM & 7.21 $\\pm$ 1.49 & 13.27 $\\pm$ 1.04 & 20.48 $\\pm$ 2.08 & 1.53 $\\pm$ 1.32 \\\\\n"
     ]
    }
   ],
   "source": [
    "print(\"Model & Method & Training time & Sampling time & Total time & KL\" + r\" \\\\\")\n",
    "print(\"\\hline\")\n",
    "print(\"\\hline\")\n",
    "row_names = {\"posthoc\":\"FSLM\", \"direct\": \"NLE\", \"default\": \"NLE\", \"1_1_00\":\"FSLM\"}\n",
    "\n",
    "# For GLM\n",
    "lines = data_db.query(\"timings_edit\").split(\" \\n\")[:-1]\n",
    "dct = {line.split(\":\")[0]:float(line.split(\": \")[1]) for line in lines}\n",
    "tags = extract_tags(dct)\n",
    "\n",
    "for i, method in enumerate([\"direct\", \"posthoc\"]):\n",
    "    for j, sampling_method in enumerate([\"rejection\"]):\n",
    "        if method == \"posthoc\" and sampling_method == \"mcmc\":\n",
    "            seeds = range(1,11)\n",
    "        else:\n",
    "            seeds = range(9)\n",
    "        times = {key:val for key, val in dct.items() if f\"{method}_{sampling_method}\" in key and \"fixed\" in key}\n",
    "\n",
    "        train_times = {key:t for key, t in times.items() if \"train\" in key}\n",
    "        train_times = torch.tensor([[val for key, val in train_times.items() if f\"_{i}_\" in key] for i in seeds])\n",
    "        sample_times = {key:t for key, t in times.items() if not \"train\" in key}\n",
    "        sample_times = torch.tensor([[val for key, val in sample_times.items() if f\"_{i}_\" in key] for i in seeds])\n",
    "        ttrain = f\"{float(train_times.sum(1).mean()) / 60 :.2f} $\\pm$ {float(train_times.sum(1).std()) / 60 :.2f}\"\n",
    "        tsample = f\"{float(sample_times.sum(1).mean()) / 60 :.2f} $\\pm$ {float(sample_times.sum(1).std()) / 60 :.2f}\"\n",
    "        ttotal = f\"{float((sample_times.sum(1) + train_times.sum(1)).mean()) / 60 :.2f} $\\pm$ {float((sample_times.sum(1) + train_times.sum(1)).std()) / 60 :.2f}\"\n",
    "        \n",
    "        acc_kls = torch.zeros(4,9)\n",
    "        for i, subset in enumerate(skip_dims([0,1,2,3])):\n",
    "            selection = {key: val for key,val in data_db.find(f\"{method}_{sampling_method}_\").items() if f\"{subset}\" in key and \"fixed\" in key and not \"_1_\" in key and not \"posterior\" in key}\n",
    "            base_sample = data_db.query(f\"direct_{sampling_method}_fixed_1_{subset}\")\n",
    "            samples = list(selection.values())\n",
    "            kls = compare_kls(samples, base_sample)\n",
    "            acc_kls[i] = kls\n",
    "        kls = f\"{float(acc_kls.mean()):.2f} $\\pm$ {float(acc_kls.std()):.2f}\"\n",
    "        \n",
    "        if (i,j) == (0,0):\n",
    "            multirow = \"\\multirow{2}{*}{GLM [min]}\"\n",
    "        else:\n",
    "            multirow = \" \"\n",
    "        print(f\"{multirow} & {row_names[method]} & {ttrain} & {tsample} & {ttotal} & {kls}\" + r\" \\\\\")\n",
    "\n",
    "print(\"\\hline\")\n",
    "print(\"\\hline\")\n",
    "\n",
    "# For HH\n",
    "lines = HH_db.query(\"timings\").split(\" \\n\")[:-1]\n",
    "dct = {line.split(\":\")[0]:float(line.split(\": \")[1]) for line in lines}\n",
    "tags = extract_tags(dct)\n",
    "subset = [0, 1, 3, 8, 13, 18, 19, 21, 22]\n",
    "\n",
    "for i, method in enumerate([\"default\", \"1_1_00\"]):\n",
    "    times = {key:val for key, val in dct.items() if f\"{method}\" in key}\n",
    "\n",
    "    train_times = {key:t for key, t in times.items() if \"train\" in key}\n",
    "    train_times = torch.tensor([[val for key, val in train_times.items() if f\"mcmc_{i}_\" in key] for i in range(10)])\n",
    "    sample_times = {key:t for key, t in times.items() if not \"train\" in key}\n",
    "    sample_times = torch.tensor([[val for key, val in sample_times.items() if f\"mcmc_{i}_\" in key] for i in range(10)])\n",
    "    ttrain = f\"{float(train_times.sum(1).mean()) / 3600 :.2f} $\\pm$ {float(train_times.sum(1).std()) / 3600 :.2f}\"\n",
    "    tsample = f\"{float(sample_times.sum(1).mean()) / 3600 :.2f} $\\pm$ {float(sample_times.sum(1).std()) / 3600 :.2f}\"\n",
    "    ttotal = f\"{float((sample_times.sum(1) + train_times.sum(1)).mean()) / 3600 :.2f} $\\pm$ {float((sample_times.sum(1) + train_times.sum(1)).std()) / 3600 :.2f}\"\n",
    "\n",
    "    # items = HH_db.find(method)\n",
    "    # samples = {key:val for key, val in items.items() if not \"posterior\" in key}\n",
    "    # subset_samples = {key:val for key, val in samples.items() if str(subset) in key and not \"mcmc_0_\" in key}\n",
    "    # subset_samples = list(subset_samples.values())\n",
    "    # base_sample = HH_db.query(f\"direct_default_mcmc_0_{subset}\")\n",
    "    # kls = compare_kls(subset_samples, base_sample)\n",
    "    # kls = f\"{float(kls.mean()):.2f} $\\pm$ {float(kls.std()):.2f}\"\n",
    "    \n",
    "    items = HH_db.find(method)\n",
    "    samples = {key:val for key, val in items.items() if not \"posterior\" in key}\n",
    "    agg_kls = torch.zeros(10,9)\n",
    "    for i, subset in enumerate(skip_dims([0, 1, 2, 3, 8, 13, 18, 19, 21, 22])):\n",
    "        subset_samples = {key:val for key, val in samples.items() if str(subset) in key and not \"mcmc_0_\" in key}\n",
    "        subset_samples = list(subset_samples.values())\n",
    "        base_sample = HH_db.query(f\"direct_default_mcmc_0_{subset}\")\n",
    "        agg_kls[i] = compare_kls(subset_samples, base_sample)\n",
    "    \n",
    "    kls = f\"{float(agg_kls.mean()):.2f} $\\pm$ {float(agg_kls.std()):.2f}\"\n",
    "\n",
    "    if i == 0:\n",
    "        multirow = \"\\multirow{2}{*}{HH [h]}\"\n",
    "    else:\n",
    "        multirow = \" \"\n",
    "    print(f\"{multirow} & {row_names[method]} & {ttrain} & {tsample} & {ttotal} & {kls}\" + r\" \\\\\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "6614bbeae8a74b664e3a9ec3cee50b04aed6b53512c2f292a1d001732c9f0da9"
  },
  "kernelspec": {
   "display_name": "Python 3.7.13 ('sbi_env')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
